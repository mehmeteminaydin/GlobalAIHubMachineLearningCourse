{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mehmet Emin AydÄ±n \n",
    "## GAIH Homework 1 Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Answer 1)\n",
    "Machine Learning is a subject of AI that is concentrated on creating applications which learn by analyzing data, at the same time, while doing so,develops itself without being explicitly programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Answer 2) \n",
    "The first difference is whether dataset is labeled or not. In a supervised learning model, the algorithm learns on a \"labeled\" dataset. On the other hand, in an unsupervised model, datasets are unlabeled. Trying to estimate how the dollar rate change in the next years can be given as an example of supervised learning model.The second difference is that while supervised learning model takes direct feedback to check if it is predicting correct output or not, unsupervised learning model does not take any feedback.The last one is that supervised learning model predicts the output which does not exist in unsupervised learning models. Instead of this, unsupervised learning model finds the hidden patterns in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Answer 3)\n",
    "Validation dataset is the sample of data which is used to supply an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. \n",
    "Test dataset is the sample of data that is used to provide an unbiased evaluation of a final model fit on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Answer 4)\n",
    "    \n",
    "    Pre-processing deals with \n",
    "        \"dublicated values\" which is about removing dublicated (unnecessary) data, \n",
    "        \"imbalanced data\" which is related to equilibrating imbalanced dataset, \n",
    "        \"missing values\" which is needed to be eliminated and replaced with mean or median, \n",
    "        \"outlier detection\" which is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data, \n",
    "        \"feature scaling\" which is a method used to normalize the range of independent variables or features of data.,\n",
    "        \"data binning\" which is a data pre-processing method used to minimize the effects of small observation errors (noisy data),\n",
    "        \"feature extraction\" which is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing,\n",
    "        \"feature encoding\" which is the process of transforming a categorical variable into a continuous variable and using them in the model.\n",
    "        \n",
    "Because we cannot fit and evaluate machine learning algorithms on raw data; instead, we must transform the data to meet the requirements of individual machine learning algorithms. More than that, we must choose a representation for the data that best exposes the unknown underlying structure of the prediction problem to the learning algorithms in order to get the best performance given our available resources on a predictive modeling project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Answer 5)\n",
    "A simple way to describe the difference between the two is to visualize a scatter plot graph vs. a line graph. Then, I check them whose data is piece by piece seperated and which one is continually drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Answer 6)\n",
    "Plot type is histogram, variable type is continous. First of all, I check whether there exists any dublicated data. Secondly, as far as I can see, there are some missing values. Therefore, we may need to eliminate them and replace them with the median values. Also, there may be some outlier values. After checking third quarters and max values, I can determine whether they are outlier or not. Then, I get rid of them. Last, I apply feature scaling on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
